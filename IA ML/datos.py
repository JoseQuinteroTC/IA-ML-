

import numpy as np
import pandas as pd
import string
import re
import nltk
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import VarianceThreshold
from scipy.spatial.distance import cosine
from unidecode import unidecode
from sklearn.tree import plot_tree

# Gr치ficos
import matplotlib.pyplot as plt
import matplotlib.style as style
import seaborn as sns
from wordcloud import WordCloud

# Preprocesado y modelado
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score


# Configuraci칩n warnings
import warnings
warnings.filterwarnings('ignore')
import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import warnings


df = pd.read_csv("train_new.txt", 
                      delimiter = '|') 
  
# storing this dataframe in a csv file 
df.to_csv('hola.csv',  
                  index = None) 



print(df.head())

# Contar el total de filas
total_filas = df.shape[0]

print("\nTotal de filas:", total_filas)

# Establecer el tama침o de la figura
plt.figure(figsize=(8, 5))

# Crear un countplot con las categor칤as "en" y "nl" de diferentes colores
sns.countplot(data=df, x='idioma', palette={'en': 'blue', 'nl': 'black'})

# Configurar los ejes y etiquetas
plt.xlabel('Idioma')
plt.ylabel('Cantidad')

# Mostrar el gr치fico
plt.show()

def limpiar_tokenizar(texto):
    # Crear instancia del lematizador
    lemmatizer = WordNetLemmatizer()
    
    nuevo_texto = texto.lower()
    # Eliminaci칩n de signos de puntuaci칩n
    regex = '[\\!\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]'
    nuevo_texto = re.sub(regex , ' ', nuevo_texto)
    # Eliminaci칩n de n칰meros
    nuevo_texto = re.sub("\d+", ' ', nuevo_texto)
    # Eliminaci칩n de espacios en blanco m칰ltiples
    nuevo_texto = re.sub("\\s+", ' ', nuevo_texto)
    # Eliminaci칩n de di칠resis
    nuevo_texto = unidecode(nuevo_texto)
    # Eliminaci칩n de fracciones
    nuevo_texto = re.sub("\d+/\d+", ' ', nuevo_texto)
    # Eliminaci칩n de fechas
    nuevo_texto = re.sub("\d{1,2}/\d{1,2}/\d{2,4}", ' ', nuevo_texto)
    # Tokenizaci칩n por palabras individuales
    nuevo_texto = nuevo_texto.split(sep=' ')
    # Lematizaci칩n
    nuevo_texto = [lemmatizer.lemmatize(token) for token in nuevo_texto]

    
    return nuevo_texto

df['texto_tokenizado'] = df['texto'].apply(lambda x: limpiar_tokenizar(x))
df[['idioma','texto','texto_tokenizado']].to_csv('hola.csv', index=False)


texto_tidy =df.explode(column='texto_tokenizado')
texto_tidy= texto_tidy.drop(columns='texto')
texto_tidy= texto_tidy.rename(columns={'texto_tokenizado':'token'})
texto_tidy.to_csv('texto_tidy.csv', index=False)

print(texto_tidy.head(10))



print("\nTotal de filas:", texto_tidy.shape[0])


# Import the required libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Convert float items to strings in the palabras list
palabras_por_categoria = texto_tidy.groupby('idioma')['token'].apply(lambda x: [str(word) for word in x])

# Create an instance of WordCloud for each category and generate the word cloud
wordclouds = {}
for categoria, palabras in palabras_por_categoria.items():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(palabras))
    wordclouds[categoria] = wordcloud

# Display the word clouds for each category
plt.figure(figsize=(12, 10))
for i, (categoria, wordcloud) in enumerate(wordclouds.items()):
    plt.subplot(1, 2, i+1)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(categoria)
    plt.axis('off')
plt.tight_layout()
plt.show()


#@title Recuento total de palabras por categor칤a
word_count = texto_tidy.groupby('idioma')['token'].count()

# Crear la tabla gr치fica
fig, ax = plt.subplots(figsize=(6, 4))
colors = ['red', 'green']  # Colores para las categor칤as 'rotten' y 'fresh'
word_count.plot(kind='bar', ax=ax, color=colors)
ax.set_xlabel('Categor칤a')
ax.set_ylabel('Recuento de palabras')
ax.set_title('Recuento total de palabras por categor칤a')

# Mostrar la tabla gr치fica
plt.show()

print('--------------------------')
print('Palabras totales por categor칤a 游녢')
print('--------------------------')
texto_tidy.groupby(by='idioma')['token'].count()
print(word_count)

# Palabras distintas utilizadas 
# ==============================================================================
print('----------------------------')
print('Palabras distintas por categoria')
print('----------------------------')
print(texto_tidy.groupby(by='idioma')['token'].nunique())


#@title Gr치fico porcentaje de palabras distintas por categor칤a
unique_words = texto_tidy.groupby('idioma')['token'].nunique()

# Definir los colores de las categor칤as
colors = ['green', 'red']

# Crear el gr치fico de pastel
fig, ax = plt.subplots()
patches, texts, autotexts = ax.pie(unique_words, labels=unique_words.index, autopct='%1.1f%%', startangle=90, colors=colors)

# Agregar los valores num칠ricos junto a las etiquetas
for i, label in enumerate(texts):
    percentage = label.get_text()
    count = unique_words[i]
    label.set_text(f'{percentage} ({count})')

ax.set_title('Palabras distintas por categor칤a')

# Mostrar el gr치fico de pastel
plt.show()



# ==============================================================================
palabras_en = set(texto_tidy[texto_tidy.idioma == 'en']['token'])
palabras_nl = set(texto_tidy[texto_tidy.idioma == 'nl']['token'])


print(f"Palabras comunes entre Ingles y Nerlandes: {len(palabras_en.intersection(palabras_nl))}")

# Guardar en un csv las palabras que estan presentes en ambos idiomas
palabras_comunes = palabras_en.intersection(palabras_nl)
palabras_comunes_lista = list(palabras_comunes)

df_palabras_comunes = pd.DataFrame(palabras_comunes_lista, columns=['Palabra'])
df_palabras_comunes.to_csv('palabras_comunes.csv', index=False)


#@title Nube las palabras comunes en las dos categorias
# Convertir los elementos de palabras_comunes a cadenas de texto
palabras_comunes_str = [str(palabra) for palabra in palabras_comunes]

# Crear una cadena de texto con las palabras comunes
texto_comun = ' '.join(palabras_comunes_str)

# Crear el objeto WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_comun)

# Mostrar la nube de palabras
plt.figure(figsize=(9, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de palabras - Palabras comunes en las dos categorias')
plt.show()


# C치lculo del log of odds ratio de cada palabra (rotten vs fresh)
# ==============================================================================
# Pivotaje y despivotaje
comentarios_pivot = texto_tidy.groupby(["idioma","token"])["token"] \
                .agg(["count"]).reset_index() \
                .pivot(index = "token" , columns="idioma", values= "count")

comentarios_pivot = comentarios_pivot.fillna(value=0)
comentarios_pivot.columns.name = None

comentarios_unpivot = comentarios_pivot.melt(value_name='n', var_name='idioma', ignore_index=False)
comentarios_unpivot = comentarios_unpivot.reset_index()

# Selecci칩n de las categorias rotten y fresh
comentarios_unpivot = comentarios_unpivot[comentarios_unpivot.idioma.isin(['en', 'nl'])]

# Se a침ade el total de palabras de cada categoria
comentarios_unpivot = comentarios_unpivot.merge(
                    texto_tidy.groupby('idioma')['token'].count().rename('N'),
                    how = 'left',
                    on  = 'idioma'
                 )

# C치lculo de odds y log of odds de cada palabra
comentarios_logOdds = comentarios_unpivot.copy()
comentarios_logOdds['odds'] = (comentarios_logOdds.n + 1) / (comentarios_logOdds.N + 1)
comentarios_logOdds = comentarios_logOdds[['token', 'idioma', 'odds']] \
                    .pivot(index='token', columns='idioma', values='odds')
comentarios_logOdds.columns.name = None

comentarios_logOdds['log_odds']     = np.log(comentarios_logOdds.en/comentarios_logOdds.nl)
comentarios_logOdds['abs_log_odds'] = np.abs(comentarios_logOdds.log_odds)

# Si el logaritmo de odds es mayor que cero, significa que es una palabra con
# mayor probabilidad de ser de rotten. Esto es as칤 porque el ratio se ha
# calculado como rotten/fresh.
comentarios_logOdds['categoria frecuente'] = np.where(comentarios_logOdds.log_odds > 0,
                                              "en",
                                              "nl"
                                    )

print('-----------------------------------')
print('Top 10 palabras m치s diferenciadoras')
print('-----------------------------------')
print(comentarios_logOdds.sort_values('abs_log_odds', ascending=False).head(15))


#@title Top 15 palabras m치s caracter칤sticas de cada categoria
# ==============================================================================

top_30 = comentarios_logOdds[['log_odds', 'abs_log_odds', 'categoria frecuente']] \
        .groupby('categoria frecuente') \
        .apply(lambda x: x.nlargest(15, columns='abs_log_odds').reset_index()) \
        .reset_index(drop=True) \
        .sort_values('log_odds')

f, ax = plt.subplots(figsize=(4, 7))
sns.barplot(
    x='log_odds',
    y='token',
    hue='categoria frecuente',
    data=top_30,
    ax=ax,
    palette={'en': 'red', 'nl': 'green'}  # Cambiar los colores aqu칤
)
ax.set_title('Top 15 palabras m치s caracter칤sticas')
ax.set_xlabel('log odds ratio (en / nl)')


# C치lculo term-frecuency (tf)
# ==============================================================================
tf = texto_tidy.copy()
# N칰mero de veces que aparece cada t칠rmino en cada comentario
tf = tf.groupby(["token"])["token"].agg(["count"]).reset_index()
# Se a침ade una columna con el total de t칠rminos por comentario
tf['total_count'] = tf.groupby('token')['count'].transform(sum)
# Se calcula el tf
tf['tf'] = tf["count"] / tf["total_count"]
print("=======================")
print(tf.sort_values(by = "tf").head(5))

# Inverse document frequency
# ==============================================================================
idf = texto_tidy.copy()
total_documents = idf["token"].drop_duplicates().count()
# N칰mero de comentarios en los que aparece cada t칠rmino
idf = idf.groupby(["token"])["token"].agg(["count"]).reset_index()
idf[' n_veces'] = idf.groupby('token')['count'].transform(sum)
# C치lculo del idf
idf['idf'] = np.log(total_documents / idf[' n_veces'])
idf = idf[["token"," n_veces", "idf"]].drop_duplicates()
print("=======================")
print(idf.sort_values(by="idf").head(5))


# Term Frequency - Inverse Document Frequency
# ==============================================================================
tf_idf = pd.merge(left=tf, right=idf, on="token")
tf_idf["tf_idf"] = tf_idf["tf"] * tf_idf["idf"]
print("=======================")
print(tf_idf.sort_values(by="token").head())






# Leer el archivo CSV
df = pd.read_csv("./hola.csv")

# Acceder a los datos de las columnas
idioma = df['idioma']
texto = df['texto']
texto_tokenizado = df['texto_tokenizado']
print("=======================")
print(df.head())


datos_X = df['texto_tokenizado']
datos_y = df['idioma']
print("=======================")
print(datos_X.unique())
print("=======================")
print(datos_y.unique())

X_train, X_test, y_train, y_test = train_test_split(
    datos_X,
    datos_y,
    test_size=0.80,
    random_state=42
)

from sklearn.feature_extraction.text import CountVectorizer

# Crear una instancia del vectorizador
vectorizer = CountVectorizer()

# Ajustar el vectorizador con los datos de entrenamiento
Xtrain = vectorizer.fit_transform(X_train)

# Transformar los datos de prueba utilizando el vectorizador ajustado
Xtest = vectorizer.transform(X_test)

print("=======================")
print(Xtrain.shape)

tokens = vectorizer.get_feature_names_out()
print("=======================")
print("Tokens:", tokens)
print("Tokens:", len(tokens))

print("=======================")
print("Conjunto de entrenamiento - Caracter칤sticas:", Xtrain.shape)
print("Conjunto de entrenamiento - Etiquetas:", y_train.shape)
print("Conjunto de prueba - Caracter칤sticas:", Xtest.shape)
print("Conjunto de prueba - Etiquetas:", y_test.shape)


from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.datasets import make_classification

def cuasiConstantes(Xtrain, Xtest, threshold=0.02):
    # Crear el objeto VarianceThreshold
    selector = VarianceThreshold(threshold=threshold)

    # Aplicar la eliminaci칩n de caracter칤sticas cuasi-constantes en la matriz tf-idf de entrenamiento
    xtrain = selector.fit_transform(Xtrain)

    # Obtener los 칤ndices de las caracter칤sticas seleccionadas
    selected_feature_indices = selector.get_support(indices=True)

    # Filtrar la matriz tf-idf de prueba utilizando los 칤ndices de caracter칤sticas seleccionadas
    xtest = selector.transform(Xtest)

    num_selected_features = xtrain.shape[1]
    print("=======================")
    print("N칰mero de caracter칤sticas seleccionadas:", num_selected_features)
    print("=======================")
    return xtrain, xtest, selected_feature_indices


def ScaracteristicasRandom(xtrain, y_train, n_estimators=100, max_depth=None, min_samples_split=2):
    # Crear un estimador Random Forest con par치metros ajustados
    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, n_jobs=-1)

    # Crear un selector RFE con Random Forest y ranking
    rfe = RFE(estimator=rf, step=2)

    # Aplicar RFE al conjunto de datos
    xtrainE = rfe.fit_transform(xtrain, y_train)
    # Obtener las caracter칤sticas seleccionadas
    selected_features = rfe.support_

    # Obtener el ranking de importancia de caracter칤sticas
    feature_ranking = rfe.ranking_

    # Imprimir las caracter칤sticas seleccionadas
    print("Caracter칤sticas seleccionadas:")
    for i, feature in enumerate(selected_features):
        if feature:
            print(f"Feature {i+1}")

    # Imprimir el ranking de caracter칤sticas
    print("Ranking de caracter칤sticas :")
    for i, rank in enumerate(feature_ranking):
        print(f"Feature {i+1}: {rank}")
        


    return xtrainE, selected_features, feature_ranking

from sklearn.feature_selection import VarianceThreshold
xtrain, xtest, selected_feature_indices=cuasiConstantes(Xtrain, Xtest)
#num_selected_features = xtrain.shape[1]

xtrainE, selected_features, feature_ranking = ScaracteristicasRandom(xtrain, y_train)
xtestE = xtest[:, selected_features]  # Conjunto de prueba transformado


# Contar caracter칤sticas seleccionadas
num_selected_features = sum(selected_features)
print("=======================")
print("N칰mero de caracter칤sticas seleccionadas:", num_selected_features)

# Crear el modelo de Random Forest
modelo_rf = RandomForestClassifier(n_estimators=500)
print("=======================")
print("Dimensiones entreamiento :", xtrain.shape)
print("Dimensiones prueba ", y_train.shape)


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rf_Model = RandomForestClassifier()
cross_val_scores = cross_val_score(rf_Model, Xtrain, y_train, cv=2)
rf_Model.fit(Xtrain, y_train)
# Evaluar el rendimiento del modelo en los datos de entrenamiento y prueba
train_accuracy = rf_Model.score(Xtrain, y_train) * 100
test_accuracy = rf_Model.score(Xtest, y_test) * 100
print("=======================")
print("Precisi칩n del entrenamiento:", "{:.3f}%".format(train_accuracy))
print("Precisi칩n de prueba:", "{:.3f}%".format(test_accuracy))

# Imprimir los resultados de la validaci칩n cruzada
print("=======================")
print("Resultados de la validaci칩n cruzada:")
print(cross_val_scores)
print("Precisi칩n media: {:.2f}".format(cross_val_scores.mean()))

# Crear el modelo de Random Forest
modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)

print("Dimensiones entreamiento :", xtrain.shape)
print("Dimensiones prueba ", y_train.shape)

# N칰mero de 치rboles en el bosque aleatorio
n_estimators = [int(x) for x in np.linspace(start=150, stop=400, num=4)]
# N칰mero de caracter칤sticas a considerar en cada divisi칩n
max_features = ['auto', 'sqrt']
# N칰mero m치ximo de niveles en el 치rbol
max_depth = None,
# N칰mero m칤nimo de muestras requeridas para dividir un nodo
min_samples_split = [5, 10]
# N칰mero m칤nimo de muestras requeridas en cada nodo hoja
min_samples_leaf = [1, 2]
# M칠todo de selecci칩n de muestras para entrenar cada 치rbol

param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,              
                }
print(param_grid)

rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 2, verbose=2, n_jobs = 4)

rf_Grid.fit(xtrain, y_train)

rf_Grid.best_params_

# Evaluar el rendimiento del modelo en los datos de entrenamiento y prueba
train_accuracy = rf_Grid.score(xtrain, y_train) * 100
test_accuracy = rf_Grid.score(xtest, y_test) * 100

print("Precisi칩n del entrenamiento:", "{:.3f}%".format(train_accuracy))
print("Precisi칩n de prueba:", "{:.3f}%".format(test_accuracy))

# Contar caracter칤sticas seleccionadas
num_selected_features = sum(selected_features)
print("N칰mero de caracter칤sticas seleccionadas:", num_selected_features)

# Crear el modelo de Random Forest
modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)

print("Dimensiones :", xtrainE.shape)
print("Dimensiones ", y_train.shape)

# N칰mero de 치rboles en el bosque aleatorio
n_estimators = [int(x) for x in np.linspace(start=100, stop=150, num=3)]
#N칰mero de caracter칤sticas a considerar en cada divisi칩n
max_features = ['auto', 'sqrt']
# N칰mero m치ximo de niveles en el 치rbol
max_depth = [2, 4]
# N칰mero m칤nimo de muestras requeridas para dividir un nodo
min_samples_split = [2, 5]
# N칰mero m칤nimo de muestras requeridas en cada nodo hoja
min_samples_leaf = [1, 2]

param_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
}

print(param_grid)

rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 2, verbose=2, n_jobs = 4)

rf_Grid.fit(xtrainE, y_train)



# Evaluar el rendimiento del modelo en los datos de entrenamiento y prueba
train_accuracy = rf_Grid.score(xtrainE, y_train) * 100
test_accuracy = rf_Grid.score(xtestE, y_test) * 100

print("Precisi칩n del entrenamiento:", "{:.3f}%".format(train_accuracy))
print("Precisi칩n de prueba:", "{:.3f}%".format(test_accuracy))


# Ajuste inicial del modelo
dt = DecisionTreeClassifier()
dt.fit(Xtrain, y_train)
y_pred_initial = dt.predict(Xtest)

# Matriz de confusi칩n inicial
dt_initial = confusion_matrix(y_test, y_pred_initial)
dt_df_initial = pd.DataFrame(dt_initial)

# Precisi칩n inicial
from sklearn.metrics import accuracy_score

dt_precision_initial = 100 * accuracy_score(y_test, y_pred_initial)
print('Precisi칩n inicial: {0:.3f}%'.format(dt_precision_initial))

from sklearn.datasets import make_classification
from sklearn.metrics import classification_report

# Generate a synthetic classification dataset

y_pred_train = dt.predict(Xtrain)
y_pred_test  = dt.predict(Xtest)

print(classification_report(y_train, y_pred_train))
print('----------------------------------------------------------')
print(classification_report(y_test, y_pred_test))







